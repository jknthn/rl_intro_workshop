{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_Q(Q, name):\n",
    "    file = open(name, 'w+')\n",
    "    for key in Q.keys():\n",
    "        line = [f\"{key}\"]\n",
    "        for value in Q[key]:\n",
    "            line.append(f\"{value}\")\n",
    "        file.write(';'.join(line) + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "def deserialize_Q(name):\n",
    "    file = open(name, 'r')\n",
    "    Q = {}\n",
    "    for line in file.readlines():\n",
    "        line_arr = line[:-1].split(';')\n",
    "        key = int(line_arr.pop(0))\n",
    "        Q[key] = np.array([float(e) for e in line_arr])\n",
    "    file.close()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_empty_dictionary(env, default_value=0.0):\n",
    "    return {k:np.ones(env.action_space.n)*default_value for k in range(env.observation_space.n)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_Q(Q):\n",
    "    return {k:np.argmax(v) for k, v in Q.items()}\n",
    "\n",
    "def epsilon_greedy_for_state_action_values(Q_s, epsilon, env):\n",
    "    policy_s = np.ones(env.action_space.n) * epsilon / env.action_space.n\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / env.action_space.n)\n",
    "    return policy_s\n",
    "\n",
    "def generate_episode(env, Q=None, epsilon=None):\n",
    "    greedy_policy = greedy_policy_from_Q(Q) if Q else None\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        if Q is None:\n",
    "            action = env.action_space.sample()\n",
    "        elif Q is not None and epsilon is not None:\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env)) if state in Q else env.action_space.sample()\n",
    "        else:\n",
    "            action = greedy_policy[state]\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_playthrough(env, Q=None, epsilon=None):\n",
    "    greedy_policy = greedy_policy_from_Q(Q) if Q else None\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    clear_output(True)\n",
    "    env.render()\n",
    "    print(rewards)\n",
    "    time.sleep(.3)\n",
    "    while True:\n",
    "        if greedy_policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        elif Q is not None and epsilon is not None:\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env)) if state in Q else env.action_space.sample()\n",
    "        else:\n",
    "            action = greedy_policy[state]\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        state = next_state\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        print(rewards)\n",
    "        time.sleep(.3)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(env, Q=None, epsilon=None):\n",
    "    greedy_policy = greedy_policy_from_Q(Q) if Q else None\n",
    "    all_rewards = []\n",
    "    for i in range(100):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            if Q is None:\n",
    "                action = env.action_space.sample()\n",
    "            elif Q is not None and epsilon is not None:\n",
    "                action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env)) if state in Q else env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_policy[state]\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                all_rewards.append(rewards)\n",
    "                break\n",
    "    return np.average(np.array(all_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = generate_empty_dictionary(env)\n",
    "    N = generate_empty_dictionary(env)\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        episode = generate_episode(env)\n",
    "        states = list(map(lambda x: x[0], episode))\n",
    "        rewards = np.array(list(map(lambda x: x[2], episode)))\n",
    "        discounts = np.array([gamma ** (i + 1) for i in range(0, len(rewards))])\n",
    "        for i in range(0, len(episode)):\n",
    "            s_i, a_i, r_i_next = episode[i]\n",
    "            if s_i not in states[0:i]:\n",
    "                N[s_i][a_i] += 1\n",
    "                returns_sum[s_i][a_i] += r_i_next + gamma * sum(rewards[i + 1:] * discounts[i + 1:])\n",
    "    \n",
    "    for state in returns_sum.keys():\n",
    "        for i in range(len(returns_sum[state])):\n",
    "            if N[state][i] != 0.0:\n",
    "                Q[state][i] = returns_sum[state][i] / N[state][i]\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "# render_playthrough(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1000/1000.\n",
      "Action values for 0 state: [0. 0. 0. 0. 0. 0.]\n",
      "Action values for 1 state: [-379.36363636 -452.41666667 -431.4        -471.64705882 -429.5\n",
      " -453.22222222]\n",
      "Action values for 2 state: [-391.92857143 -419.2        -512.42857143 -393.125      -540.2\n",
      " -411.33333333]\n",
      "Action values for 3 state: [-460.875      -395.73333333 -418.58333333 -510.14285714 -439.71428571\n",
      " -383.        ]\n",
      "Action values for 4 state: [-471.5        -371.53333333 -663.66666667 -576.84615385 -465.28571429\n",
      " -537.42857143]\n",
      "Action values for 5 state: [0. 0. 0. 0. 0. 0.]\n",
      "Action values for 6 state: [-610.66666667 -410.57142857 -588.75       -557.22222222 -413.42857143\n",
      " -493.6       ]\n",
      "Action values for 7 state: [-423.9        -662.375      -334.5        -634.5        -521.\n",
      " -592.83333333]\n",
      "Action values for 8 state: [-586.88888889 -281.18181818 -489.77777778 -650.         -377.875\n",
      " -572.55555556]\n",
      "Action values for 9 state: [-556.14285714 -482.75       -479.625      -337.2        -488.\n",
      " -535.70588235]\n"
     ]
    }
   ],
   "source": [
    "Q = mc_prediction_q(env, 1000)\n",
    "for i in range(10):\n",
    "    print(f\"Action values for {i} state: {Q[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000/3000.\n",
      "-935.75\n",
      "-1077.23\n",
      "-990.02\n"
     ]
    }
   ],
   "source": [
    "Q_1 = mc_prediction_q(env, 1000)\n",
    "Q_2 = mc_prediction_q(env, 2000)\n",
    "Q_3 = mc_prediction_q(env, 3000)\n",
    "\n",
    "print(benchmark(env, Q_1))\n",
    "print(benchmark(env, Q_2))\n",
    "print(benchmark(env, Q_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, eps=0.1, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    # loop over episodes\n",
    "    eps_a = 1.0\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        eps_a = max(eps_a * 0.9999, eps)\n",
    "        episode = generate_episode(env, Q, eps_a)\n",
    "        \n",
    "        states = list(map(lambda x: x[0], episode))\n",
    "        rewards = np.array(list(map(lambda x: x[2], episode)))\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        \n",
    "        for i in range(0, len(episode)):\n",
    "            s_i, a_i, r_i_next = episode[i]\n",
    "            if s_i not in states[0:i]:\n",
    "                Q[s_i][a_i] = Q[s_i][a_i] + alpha * (sum(rewards[i:]*discounts[:-(1+i)]) - Q[s_i][a_i])\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 10000/10000.\n"
     ]
    }
   ],
   "source": [
    "Q = mc_control(env, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-446.73\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(env, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "-15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d422badf8651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrender_playthrough\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-c676f35d6548>\u001b[0m in \u001b[0;36mrender_playthrough\u001b[0;34m(env, Q, epsilon)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "render_playthrough(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Learning (Sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        state = env.reset()  \n",
    "        epsilon = 1.0 / i_episode\n",
    "        action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env))\n",
    "\n",
    "        for t_step in np.arange(300):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if not done:\n",
    "                next_action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[next_state], epsilon, env))\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * Q[next_state][next_action]) - Q[state][action])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            if done:\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * 0) - Q[state][action])\n",
    "                break\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 50000/50000.\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa(env, 50000, 0.05, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.74\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(env, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "render_playthrough(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Learning (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        state = env.reset()  \n",
    "        epsilon = 1.0 / i_episode\n",
    "        action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env))\n",
    "\n",
    "        for t_step in np.arange(300):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if not done:\n",
    "                next_action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[next_state], epsilon, env))\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * np.amax(Q[next_state])) - Q[state][action])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            if done:\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * 0) - Q[state][action])\n",
    "                break\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 50000/50000.\n"
     ]
    }
   ],
   "source": [
    "Q = q_learning(env, 50000, 0.05, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.56\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(env, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "render_playthrough(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
