{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_Q(Q, name):\n",
    "    file = open(name, 'w+')\n",
    "    for key in Q.keys():\n",
    "        line = [f\"{key}\"]\n",
    "        for value in Q[key]:\n",
    "            line.append(f\"{value}\")\n",
    "        file.write(';'.join(line) + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "def deserialize_Q(name):\n",
    "    file = open(name, 'r')\n",
    "    Q = {}\n",
    "    for line in file.readlines():\n",
    "        line_arr = line[:-1].split(';')\n",
    "        key = int(line_arr.pop(0))\n",
    "        Q[key] = np.array([float(e) for e in line_arr])\n",
    "    file.close()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_empty_dictionary(env, default_value=0.0):\n",
    "    return {k:np.ones(env.action_space.n)*default_value for k in range(env.observation_space.n)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_Q(Q):\n",
    "    return {k:np.argmax(v) for k, v in Q.items()}\n",
    "\n",
    "def epsilon_greedy_for_state_action_values(Q_s, epsilon, env):\n",
    "    policy_s = np.ones(env.action_space.n) * epsilon / env.action_space.n\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / env.action_space.n)\n",
    "    return policy_s\n",
    "\n",
    "def generate_episode(env, Q=None, epsilon=None):\n",
    "    greedy_policy = greedy_policy_from_Q(Q) if Q else None\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        if Q is None:\n",
    "            action = env.action_space.sample()\n",
    "        elif Q is not None and epsilon is not None:\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env)) if state in Q else env.action_space.sample()\n",
    "        else:\n",
    "            action = greedy_policy[state]\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_playthrough(env, Q=None, epsilon=None):\n",
    "    greedy_policy = greedy_policy_from_Q(Q) if Q else None\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    clear_output(True)\n",
    "    env.render()\n",
    "    time.sleep(.3)\n",
    "    while True:\n",
    "        if greedy_policy is None:\n",
    "            action = env.action_space.sample()\n",
    "        elif Q is not None and epsilon is not None:\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env)) if state in Q else env.action_space.sample()\n",
    "        else:\n",
    "            action = greedy_policy[state]\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        state = next_state\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        time.sleep(.3)\n",
    "        if done:\n",
    "            print(rewards)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(env, Q=None, epsilon=None):\n",
    "    greedy_policy = greedy_policy_from_Q(Q) if Q else None\n",
    "    all_rewards = []\n",
    "    for i in range(100):\n",
    "        state = env.reset()\n",
    "        rewards = 0\n",
    "        while True:\n",
    "            if Q is None:\n",
    "                action = env.action_space.sample()\n",
    "            elif Q is not None and epsilon is not None:\n",
    "                action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env)) if state in Q else env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_policy[state]\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            rewards += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                all_rewards.append(rewards)\n",
    "                break\n",
    "    return np.average(np.array(all_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = generate_empty_dictionary(env)\n",
    "    N = generate_empty_dictionary(env)\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        episode = generate_episode(env)\n",
    "        states = list(map(lambda x: x[0], episode))\n",
    "        rewards = np.array(list(map(lambda x: x[2], episode)))\n",
    "        discounts = np.array([gamma ** (i + 1) for i in range(0, len(rewards))])\n",
    "        for i in range(0, len(episode)):\n",
    "            s_i, a_i, r_i_next = episode[i]\n",
    "            if s_i not in states[0:i]:\n",
    "                N[s_i][a_i] += 1\n",
    "                returns_sum[s_i][a_i] += r_i_next + gamma * sum(rewards[i + 1:] * discounts[i + 1:])\n",
    "    \n",
    "    for state in returns_sum.keys():\n",
    "        for i in range(len(returns_sum[state])):\n",
    "            if N[state][i] != 0.0:\n",
    "                Q[state][i] = returns_sum[state][i] / N[state][i]\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "-623\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v2')\n",
    "render_playthrough(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 10000/10000.\n",
      "Action values for 0 state: [0. 0. 0. 0. 0. 0.]\n",
      "Action values for 1 state: [-492.74107143 -420.60869565 -427.84       -464.63963964 -406.37614679\n",
      " -425.39583333]\n",
      "Action values for 2 state: [-474.52777778 -431.76033058 -432.42056075 -413.375      -422.31192661\n",
      " -455.10091743]\n",
      "Action values for 3 state: [-469.86238532 -434.71698113 -459.67592593 -432.75384615 -450.35964912\n",
      " -468.64285714]\n",
      "Action values for 4 state: [-549.52380952 -505.60576923 -506.18556701 -495.21212121 -526.75\n",
      " -491.18292683]\n",
      "Action values for 5 state: [0. 0. 0. 0. 0. 0.]\n",
      "Action values for 6 state: [-548.12698413 -463.67045455 -527.26804124 -504.66346154 -496.07058824\n",
      " -517.52808989]\n",
      "Action values for 7 state: [-554.08045977 -510.63809524 -532.05405405 -537.57692308 -531.36\n",
      " -504.58888889]\n",
      "Action values for 8 state: [-480.55789474 -482.54761905 -486.97674419 -465.71287129 -489.39473684\n",
      " -476.67619048]\n",
      "Action values for 9 state: [-477.74736842 -498.1686747  -480.94230769 -501.31578947 -465.88372093\n",
      " -526.64935065]\n"
     ]
    }
   ],
   "source": [
    "Q = mc_prediction_q(env, 10000)\n",
    "for i in range(10):\n",
    "    print(f\"Action values for {i} state: {Q[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3000/3000.\n",
      "-1133.21\n",
      "-1150.94\n",
      "-953.57\n"
     ]
    }
   ],
   "source": [
    "Q_1 = mc_prediction_q(env, 1000)\n",
    "Q_2 = mc_prediction_q(env, 2000)\n",
    "Q_3 = mc_prediction_q(env, 3000)\n",
    "\n",
    "print(benchmark(env, Q_1))\n",
    "print(benchmark(env, Q_2))\n",
    "print(benchmark(env, Q_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, alpha, eps=0.1, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    # loop over episodes\n",
    "    eps_a = 1.0\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        eps_a = max(eps_a * 0.9999, eps)\n",
    "        episode = generate_episode(env, Q, eps_a)\n",
    "        \n",
    "        states = list(map(lambda x: x[0], episode))\n",
    "        rewards = np.array(list(map(lambda x: x[2], episode)))\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        \n",
    "        for i in range(0, len(episode)):\n",
    "            s_i, a_i, r_i_next = episode[i]\n",
    "            if s_i not in states[0:i]:\n",
    "                Q[s_i][a_i] = Q[s_i][a_i] + alpha * (sum(rewards[i:]*discounts[:-(1+i)]) - Q[s_i][a_i])\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 10000/10000.\n"
     ]
    }
   ],
   "source": [
    "Q = mc_control(env, 10000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-400.03\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(env, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "-200\n"
     ]
    }
   ],
   "source": [
    "render_playthrough(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Learning (Sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        state = env.reset()  \n",
    "        epsilon = 1.0 / i_episode\n",
    "        action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env))\n",
    "\n",
    "        for t_step in np.arange(300):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if not done:\n",
    "                next_action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[next_state], epsilon, env))\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * Q[next_state][next_action]) - Q[state][action])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            if done:\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * 0) - Q[state][action])\n",
    "                break\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 200000/200000.\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa(env, 200000, 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.24\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(env, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "render_playthrough(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Learning (Q-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    Q = generate_empty_dictionary(env)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            clear_output(True)\n",
    "            print(\"\\rEpisode {}/{}.\\n\".format(i_episode, num_episodes), end=\"\")\n",
    "        \n",
    "        state = env.reset()  \n",
    "        epsilon = 1.0 / i_episode\n",
    "        action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[state], epsilon, env))\n",
    "\n",
    "        for t_step in np.arange(300):\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if not done:\n",
    "                next_action = np.random.choice(np.arange(env.action_space.n), p=epsilon_greedy_for_state_action_values(Q[next_state], epsilon, env))\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * np.amax(Q[next_state])) - Q[state][action])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "            if done:\n",
    "                Q[state][action] = Q[state][action] + alpha * (reward + (gamma * 0) - Q[state][action])\n",
    "                break\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 200000/200000.\n"
     ]
    }
   ],
   "source": [
    "Q = q_learning(env, 200000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.63\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(env, Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "render_playthrough(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
